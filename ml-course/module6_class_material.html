<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Module 6: Ensemble Learning - Boosting Algorithms</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', sans-serif;
            line-height: 1.8;
            color: #2c3e50;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            padding: 40px 20px;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 20px;
            box-shadow: 0 20px 60px rgba(0, 0, 0, 0.3);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #1e3c72 0%, #2a5298 100%);
            color: white;
            padding: 60px 40px;
            text-align: center;
        }
        
        .module-number {
            font-size: 1rem;
            font-weight: 600;
            letter-spacing: 2px;
            opacity: 0.9;
            margin-bottom: 10px;
        }
        
        .header h1 {
            font-size: 3rem;
            margin-bottom: 20px;
            font-weight: 700;
        }
        
        .subtitle {
            font-size: 1.3rem;
            opacity: 0.9;
            font-weight: 300;
        }
        
        .content {
            padding: 60px 40px;
        }
        
        .section {
            margin-bottom: 60px;
        }
        
        .section-title {
            font-size: 2rem;
            color: #2c3e50;
            margin-bottom: 30px;
            padding-bottom: 15px;
            border-bottom: 3px solid #667eea;
            font-weight: 700;
        }
        
        .part-container {
            background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
            padding: 40px;
            border-radius: 15px;
            margin: 30px 0;
            border-left: 5px solid #667eea;
        }
        
        .part-title {
            font-size: 1.8rem;
            color: #1e3c72;
            margin-bottom: 20px;
            font-weight: 600;
        }
        
        .concept-box {
            background: white;
            padding: 30px;
            border-radius: 10px;
            margin: 20px 0;
            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.1);
        }
        
        .visualization-container {
            background: white;
            padding: 30px;
            border-radius: 10px;
            margin: 20px 0;
        }
        
        canvas {
            width: 100% !important;
            height: auto !important;
            max-height: 500px;
            border-radius: 10px;
            margin: 20px 0;
        }
        
        .code-block {
            background: #1e1e1e;
            color: #d4d4d4;
            padding: 25px;
            border-radius: 10px;
            margin: 20px 0;
            overflow-x: auto;
            font-family: 'Courier New', monospace;
            font-size: 0.95rem;
            line-height: 1.6;
        }
        
        .code-explanation {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            margin: 15px 0;
            border-left: 4px solid #667eea;
        }
        
        .why-box {
            background: #fff3e0;
            padding: 20px;
            border-radius: 8px;
            margin: 15px 0;
            border-left: 4px solid #ff9800;
        }
        
        .real-world-example {
            background: linear-gradient(135deg, #e3f2fd 0%, #bbdefb 100%);
            padding: 30px;
            border-radius: 10px;
            margin: 25px 0;
        }
        
        .key-insight {
            background: #e8f5e9;
            padding: 20px;
            border-radius: 8px;
            margin: 20px 0;
            border-left: 4px solid #4caf50;
        }
        
        .comparison-table {
            width: 100%;
            border-collapse: collapse;
            margin: 25px 0;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
        }
        
        .comparison-table th {
            background: #1e3c72;
            color: white;
            padding: 15px;
            text-align: left;
            font-weight: 600;
        }
        
        .comparison-table td {
            padding: 15px;
            border-bottom: 1px solid #ddd;
        }
        
        .comparison-table tr:nth-child(even) {
            background: #f8f9fa;
        }
        
        .interactive-controls {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin: 25px 0;
        }
        
        .control-item {
            background: white;
            padding: 20px;
            border-radius: 10px;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }
        
        .slider {
            width: 100%;
            -webkit-appearance: none;
            height: 8px;
            border-radius: 5px;
            background: #ddd;
            outline: none;
            margin: 10px 0;
        }
        
        .slider::-webkit-slider-thumb {
            -webkit-appearance: none;
            width: 20px;
            height: 20px;
            border-radius: 50%;
            background: #667eea;
            cursor: pointer;
        }
        
        button {
            background: #667eea;
            color: white;
            border: none;
            padding: 12px 28px;
            border-radius: 6px;
            cursor: pointer;
            font-size: 1rem;
            font-weight: 500;
            transition: all 0.3s ease;
        }
        
        button:hover {
            background: #5568d3;
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(102, 126, 234, 0.4);
        }
        
        .metric-cards {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin: 30px 0;
        }
        
        .metric-card {
            background: white;
            padding: 25px;
            border-radius: 10px;
            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.1);
            text-align: center;
            transition: transform 0.3s ease;
        }
        
        .metric-card:hover {
            transform: translateY(-5px);
        }
        
        .metric-value {
            font-size: 2.5rem;
            font-weight: 700;
            color: #667eea;
        }
        
        .metric-label {
            color: #666;
            margin-top: 10px;
            font-size: 0.95rem;
        }
        
        ul, ol {
            margin-left: 25px;
            margin-top: 15px;
        }
        
        li {
            margin: 10px 0;
        }
        
        strong {
            color: #1e3c72;
            font-weight: 600;
        }
        
        h3 {
            color: #2c3e50;
            font-size: 1.5rem;
            margin: 25px 0 15px;
            font-weight: 600;
        }
        
        h4 {
            color: #1e3c72;
            font-size: 1.2rem;
            margin: 20px 0 10px;
            font-weight: 600;
        }
        
        @media (max-width: 768px) {
            .header h1 {
                font-size: 2rem;
            }
            .content {
                padding: 30px 20px;
            }
            .interactive-controls {
                grid-template-columns: 1fr;
            }
        }
    </style>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
</head>
<body>
    <div class="container">
        <div class="header">
            <div class="module-number">MODULE 6</div>
            <h1>ðŸš€ Ensemble Learning: Boosting Algorithms</h1>
            <p class="subtitle">Transform Weak Learners into Powerful Predictors</p>
        </div>
        
        <div class="content">
            <!-- Introduction -->
            <div class="section">
                <h2 class="section-title">Course Overview</h2>
                <div class="concept-box">
                    <p><strong>Learning Objectives:</strong></p>
                    <ul>
                        <li>Understand the fundamental principle of boosting algorithms</li>
                        <li>Master AdaBoost, Gradient Boosting, and XGBoost implementations</li>
                        <li>Apply boosting techniques to real-world business problems</li>
                        <li>Compare boosting with other ensemble methods</li>
                        <li>Optimize hyperparameters for maximum performance</li>
                    </ul>
                </div>
                
                <div class="real-world-example">
                    <h3>Why This Matters: Real Business Impact</h3>
                    <p>In 2006, Netflix launched the Netflix Prize competition offering $1 million for improving their recommendation system by 10%. The winning team used ensemble methods including boosting algorithms, demonstrating how combining multiple weak models can outperform any single strong model. Today, boosting algorithms power fraud detection systems at major banks, credit scoring models, medical diagnosis tools, and recommendation engines across the tech industry.</p>
                </div>
            </div>
            
            <!-- PART 1: GENERAL CONCEPT -->
            <div class="section">
                <h2 class="section-title">Part 1: Understanding Boosting - The General Concept</h2>
                
                <div class="part-container">
                    <h3 class="part-title">What is Boosting?</h3>
                    
                    <div class="concept-box">
                        <p>Boosting is a powerful ensemble learning technique that combines multiple weak learners (models that perform slightly better than random guessing) into a single strong learner. Unlike Random Forest which builds trees independently, boosting builds them sequentially, with each new model focusing on correcting the errors made by previous models.</p>
                        
                        <p><strong>The Core Principle:</strong> Learn from mistakes iteratively. Each subsequent model pays more attention to the examples that previous models misclassified or predicted poorly.</p>
                    </div>
                    
                    <h4>The Three Pillars of Boosting</h4>
                    
                    <div class="concept-box">
                        <p><strong>1. Sequential Learning</strong></p>
                        <p>Models are built one after another, not in parallel. Each model tries to correct the mistakes of its predecessors. This sequential nature allows later models to focus computational resources on the hardest examples.</p>
                    </div>
                    
                    <div class="concept-box">
                        <p><strong>2. Weighted Examples</strong></p>
                        <p>Training examples that are misclassified receive higher weights, forcing the next model to pay more attention to them. This adaptive reweighting is what makes boosting so powerful at handling difficult cases.</p>
                    </div>
                    
                    <div class="concept-box">
                        <p><strong>3. Model Combination</strong></p>
                        <p>The final prediction combines all models' predictions through weighted voting (classification) or weighted averaging (regression). Models that perform better receive higher weights in the final decision.</p>
                    </div>
                    
                    <h4>Real-World Analogy: The Panel of Experts</h4>
                    
                    <div class="real-world-example">
                        <p>Imagine you're assembling a medical diagnosis team. Your first doctor reviews all patient cases and makes diagnoses. Then you bring in a second doctor who specifically studies the cases the first doctor got wrong or was uncertain about. A third doctor then focuses on the remaining difficult cases. Finally, when a new patient arrives, all doctors vote on the diagnosis, but doctors who have proven more accurate get more weight in the final decision. This is exactly how boosting works.</p>
                    </div>
                    
                    <h4>The Evolution of Boosting Algorithms</h4>
                    
                    <table class="comparison-table">
                        <thead>
                            <tr>
                                <th>Algorithm</th>
                                <th>Year</th>
                                <th>Key Innovation</th>
                                <th>Best Used For</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>AdaBoost</strong></td>
                                <td>1995</td>
                                <td>Adaptive sample weighting</td>
                                <td>Binary classification, face detection</td>
                            </tr>
                            <tr>
                                <td><strong>Gradient Boosting</strong></td>
                                <td>1999</td>
                                <td>Gradient descent optimization</td>
                                <td>Regression, ranking problems</td>
                            </tr>
                            <tr>
                                <td><strong>XGBoost</strong></td>
                                <td>2014</td>
                                <td>Regularization + parallelization</td>
                                <td>Kaggle competitions, production systems</td>
                            </tr>
                            <tr>
                                <td><strong>LightGBM</strong></td>
                                <td>2017</td>
                                <td>Histogram-based learning</td>
                                <td>Large datasets, fast training</td>
                            </tr>
                            <tr>
                                <td><strong>CatBoost</strong></td>
                                <td>2017</td>
                                <td>Categorical feature handling</td>
                                <td>Datasets with many categorical variables</td>
                            </tr>
                        </tbody>
                    </table>
                    
                    <div class="key-insight">
                        <p><strong>Key Insight:</strong> While Random Forest asks "what if we build many trees independently and average them?", Boosting asks "what if we build trees sequentially, each one learning from the previous one's mistakes?" This fundamental difference in approach leads to boosting typically achieving higher accuracy but requiring more careful tuning to avoid overfitting.</p>
                    </div>
                </div>
            </div>
            
            <!-- PART 2: VISUALIZATION -->
            <div class="section">
                <h2 class="section-title">Part 2: Visualizing Boosting in Action</h2>
                
                <div class="part-container">
                    <h3 class="part-title">See How Boosting Learns from Mistakes</h3>
                    
                    <div class="concept-box">
                        <p>Understanding boosting through visualization helps solidify the concept. In this interactive section, you will observe how boosting algorithms progressively improve predictions by focusing on difficult examples. Each iteration builds a new model that specifically targets the errors made by previous models, leading to a powerful ensemble predictor.</p>
                    </div>
                    
                    <h4>Interactive Demo: Boosting Learning Process</h4>
                    
                    <div class="visualization-container">
                        <canvas id="boostingVisualization"></canvas>
                        
                        <div class="interactive-controls">
                            <div class="control-item">
                                <label><strong>Number of Estimators:</strong></label>
                                <input type="range" class="slider" id="numEstimators" min="1" max="50" value="10">
                                <p id="estimatorValue" style="text-align: center; font-weight: bold; color: #667eea; margin-top: 10px;">10</p>
                            </div>
                            
                            <div class="control-item">
                                <label><strong>Learning Rate:</strong></label>
                                <input type="range" class="slider" id="learningRate" min="0.01" max="1" step="0.01" value="0.1">
                                <p id="learningRateValue" style="text-align: center; font-weight: bold; color: #667eea; margin-top: 10px;">0.1</p>
                            </div>
                            
                            <div class="control-item">
                                <label><strong>Max Depth:</strong></label>
                                <input type="range" class="slider" id="maxDepth" min="1" max="10" value="3">
                                <p id="maxDepthValue" style="text-align: center; font-weight: bold; color: #667eea; margin-top: 10px;">3</p>
                            </div>
                        </div>
                        
                        <button onclick="trainBoostingModel()" style="margin: 20px auto; display: block;">Run Boosting Algorithm</button>
                        
                        <div class="metric-cards">
                            <div class="metric-card">
                                <div class="metric-value" id="trainAccuracy">--</div>
                                <div class="metric-label">Training Accuracy</div>
                            </div>
                            <div class="metric-card">
                                <div class="metric-value" id="testAccuracy">--</div>
                                <div class="metric-label">Test Accuracy</div>
                            </div>
                            <div class="metric-card">
                                <div class="metric-value" id="iterations">--</div>
                                <div class="metric-label">Iterations Completed</div>
                            </div>
                        </div>
                    </div>
                    
                    <h4>Understanding the Visualization</h4>
                    
                    <div class="concept-box">
                        <p>The visualization above demonstrates how boosting algorithms improve over iterations. Initially, the model makes many errors. With each subsequent iteration, the algorithm identifies the misclassified points (shown in red) and builds a new weak learner that focuses specifically on getting those difficult examples correct. The decision boundary becomes progressively more sophisticated, adapting to capture complex patterns in the data.</p>
                        
                        <p>Notice how increasing the number of estimators generally improves accuracy, but there is a point of diminishing returns where additional models provide minimal benefit. The learning rate controls how much each tree contributes to the final model. A lower learning rate requires more trees but often produces better generalization. The maximum depth parameter controls tree complexity, with deeper trees capturing more intricate patterns but risking overfitting.</p>
                    </div>
                    
                    <h4>Comparing Boosting with Random Forest</h4>
                    
                    <div class="visualization-container">
                        <canvas id="comparisonChart"></canvas>
                        <button onclick="showComparison()" style="margin: 20px auto; display: block;">Show Random Forest vs Boosting Comparison</button>
                    </div>
                    
                    <div class="key-insight">
                        <p><strong>Critical Observation:</strong> Random Forest builds all trees independently and combines them through simple averaging. This parallel construction makes it fast and resistant to overfitting. Boosting builds trees sequentially, with each tree specifically targeting the previous errors. This sequential nature makes boosting more accurate on average but requires careful tuning to prevent overfitting to noise in the training data.</p>
                    </div>
                </div>
            </div>
            
            <!-- PART 3: CODE EXPLANATION -->
            <div class="section">
                <h2 class="section-title">Part 3: Implementing Boosting - Code Explanation</h2>
                
                <div class="part-container">
                    <h3 class="part-title">From Concept to Code: Building Your First Boosting Model</h3>
                    
                    <div class="concept-box">
                        <p>Understanding how to implement boosting algorithms is essential for applying them to real-world problems. In this section, we will walk through implementing three major boosting algorithms: AdaBoost, Gradient Boosting, and XGBoost. For each algorithm, we will explain what the code does, how it achieves the boosting principle, and why specific design choices matter for performance.</p>
                    </div>
                    
                    <h4>Implementation 1: AdaBoost (Adaptive Boosting)</h4>
                    
                    <div class="concept-box">
                        <p>AdaBoost was the first successful boosting algorithm and remains popular for its simplicity and effectiveness. It works by maintaining a weight for each training example. Initially, all weights are equal. After each iteration, examples that were misclassified receive increased weights, forcing the next weak learner to pay more attention to these difficult cases. The final model combines all weak learners through weighted voting.</p>
                    </div>
                    
                    <div class="code-block"># Import necessary libraries
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_classification
from sklearn.metrics import accuracy_score, classification_report
import numpy as np

# Generate synthetic dataset for demonstration
X, y = make_classification(
    n_samples=1000,
    n_features=20,
    n_informative=15,
    n_redundant=5,
    random_state=42
)

# Split into training and test sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Create base estimator (weak learner)
# Using shallow decision trees as weak learners
base_estimator = DecisionTreeClassifier(max_depth=1)

# Initialize AdaBoost classifier
adaboost = AdaBoostClassifier(
    estimator=base_estimator,
    n_estimators=50,
    learning_rate=1.0,
    random_state=42
)

# Train the model
adaboost.fit(X_train, y_train)

# Make predictions
y_pred = adaboost.predict(X_test)

# Evaluate performance
accuracy = accuracy_score(y_test, y_pred)
print(f"AdaBoost Accuracy: {accuracy:.4f}")
print("\nClassification Report:")
print(classification_report(y_test, y_pred))</div>
                    
                    <div class="code-explanation">
                        <h4>What This Code Does:</h4>
                        <p>This implementation creates an AdaBoost classifier that combines 50 decision stumps (trees with max_depth=1) into a powerful ensemble model. The classifier iteratively trains weak learners, with each subsequent learner focusing more on examples that previous learners misclassified. The final model makes predictions by weighted voting across all weak learners.</p>
                    </div>
                    
                    <div class="code-explanation">
                        <h4>How It Works:</h4>
                        <p>The AdaBoost algorithm maintains a weight for each training example. Initially, all examples have equal weight. After training the first weak learner, the algorithm identifies misclassified examples and increases their weights. When training the second weak learner, these higher-weighted examples have more influence on the loss function, forcing the new learner to focus on getting them correct. This process repeats for all 50 estimators. The learning_rate parameter controls how much weight each weak learner receives in the final ensemble. A learning rate of 1.0 means each learner contributes fully based on its training accuracy.</p>
                    </div>
                    
                    <div class="why-box">
                        <h4>Why We Code It This Way:</h4>
                        <p>Using decision stumps (max_depth=1) as base estimators is a deliberate choice. These extremely simple models are weak learners by design, meaning they perform only slightly better than random guessing on their own. However, this simplicity is actually an advantage in boosting. Because each stump is so simple, it is unlikely to overfit. The boosting algorithm can safely add many such stumps without worrying about each one memorizing the training data. The combination of many simple models creates a complex decision boundary that generalizes well to new data.</p>
                        
                        <p>The choice of 50 estimators balances accuracy with computational cost. Too few estimators leave performance on the table, while too many can lead to overfitting and unnecessary computation. In practice, you would use cross-validation to determine the optimal number of estimators for your specific dataset. The learning_rate=1.0 allows each weak learner to contribute fully, which works well when you have a moderate number of estimators. If you increase the number of estimators, you would typically decrease the learning rate to prevent the model from becoming too complex too quickly.</p>
                    </div>
                    
                    <h4>Implementation 2: Gradient Boosting</h4>
                    
                    <div class="concept-box">
                        <p>Gradient Boosting generalizes AdaBoost by using gradient descent in function space. Instead of reweighting examples, each new tree is trained to predict the residual errors (negative gradients) of the current ensemble. This approach is more flexible and typically achieves better performance, especially for regression problems. The sequential addition of trees, each correcting the errors of the previous ensemble, creates a powerful predictor.</p>
                    </div>
                    
                    <div class="code-block"># Import Gradient Boosting
from sklearn.ensemble import GradientBoostingClassifier

# Initialize Gradient Boosting classifier
gb_classifier = GradientBoostingClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3,
    min_samples_split=2,
    min_samples_leaf=1,
    subsample=0.8,
    random_state=42
)

# Train the model
gb_classifier.fit(X_train, y_train)

# Make predictions
y_pred_gb = gb_classifier.predict(X_test)
y_pred_proba = gb_classifier.predict_proba(X_test)

# Evaluate performance
gb_accuracy = accuracy_score(y_test, y_pred_gb)
print(f"Gradient Boosting Accuracy: {gb_accuracy:.4f}")

# Feature importance
feature_importance = gb_classifier.feature_importances_
print("\nTop 5 Most Important Features:")
top_indices = np.argsort(feature_importance)[-5:][::-1]
for idx in top_indices:
    print(f"Feature {idx}: {feature_importance[idx]:.4f}")</div>
                    
                    <div class="code-explanation">
                        <h4>What This Code Does:</h4>
                        <p>This implementation creates a Gradient Boosting classifier that builds an ensemble of 100 decision trees with depth 3. Unlike AdaBoost which uses extremely shallow trees, Gradient Boosting uses slightly deeper trees to capture more complex patterns. The model trains each tree to predict the residual errors of the current ensemble, progressively reducing prediction error across iterations. The subsample parameter introduces randomness by training each tree on 80% of the data, which helps prevent overfitting.</p>
                    </div>
                    
                    <div class="code-explanation">
                        <h4>How It Works:</h4>
                        <p>Gradient Boosting treats boosting as an optimization problem. The algorithm starts with a simple prediction (often just the mean of the target variable) and then adds trees one at a time. Each new tree is trained to predict the negative gradient of the loss function with respect to the current predictions. In simpler terms, each tree learns to correct the mistakes of the existing ensemble. The learning_rate parameter shrinks the contribution of each tree, which acts as regularization and improves generalization. A learning rate of 0.1 means each tree contributes only 10% of its prediction to the final model, requiring more trees but producing better results.</p>
                    </div>
                    
                    <div class="why-box">
                        <h4>Why We Code It This Way:</h4>
                        <p>The choice of max_depth=3 creates trees that can model three-way interactions between features. This is deeper than AdaBoost's stumps but still shallow enough to avoid overfitting. Each tree can capture moderately complex patterns without memorizing noise in the training data. The min_samples_split=2 and min_samples_leaf=1 parameters control tree growth, preventing overly specific splits that would only apply to a handful of examples.</p>
                        
                        <p>The subsample=0.8 parameter is particularly important for practical applications. By training each tree on only 80% of the data (randomly sampled), we introduce stochastic gradient boosting. This randomness serves two purposes. First, it reduces overfitting by preventing trees from perfectly fitting the training data. Second, it speeds up training since each tree sees fewer examples. The combination of 100 trees with a 0.1 learning rate provides strong performance while maintaining good generalization. This configuration has proven successful across many Kaggle competitions and production systems.</p>
                    </div>
                    
                    <h4>Implementation 3: XGBoost (Extreme Gradient Boosting)</h4>
                    
                    <div class="concept-box">
                        <p>XGBoost represents the state-of-the-art in boosting algorithms, combining the principles of Gradient Boosting with sophisticated optimizations for speed and performance. It introduces regularization terms to prevent overfitting, uses a more sophisticated tree-building algorithm, and implements parallel processing to dramatically speed up training. XGBoost has dominated machine learning competitions and is widely deployed in production systems at major tech companies.</p>
                    </div>
                    
                    <div class="code-block"># Import XGBoost
import xgboost as xgb
from sklearn.metrics import roc_auc_score

# Prepare data in XGBoost format
dtrain = xgb.DMatrix(X_train, label=y_train)
dtest = xgb.DMatrix(X_test, label=y_test)

# Set parameters
params = {
    'objective': 'binary:logistic',
    'max_depth': 3,
    'learning_rate': 0.1,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'min_child_weight': 1,
    'gamma': 0,
    'reg_alpha': 0.1,
    'reg_lambda': 1.0,
    'eval_metric': 'auc',
    'seed': 42
}

# Train model with early stopping
evals = [(dtrain, 'train'), (dtest, 'test')]
model = xgb.train(
    params,
    dtrain,
    num_boost_round=100,
    evals=evals,
    early_stopping_rounds=10,
    verbose_eval=False
)

# Make predictions
y_pred_xgb = (model.predict(dtest) > 0.5).astype(int)
y_pred_proba_xgb = model.predict(dtest)

# Evaluate performance
xgb_accuracy = accuracy_score(y_test, y_pred_xgb)
xgb_auc = roc_auc_score(y_test, y_pred_proba_xgb)

print(f"XGBoost Accuracy: {xgb_accuracy:.4f}")
print(f"XGBoost AUC: {xgb_auc:.4f}")

# Get feature importance
importance_dict = model.get_score(importance_type='gain')
print("\nTop Features by Importance Gain:")
sorted_importance = sorted(importance_dict.items(), 
                          key=lambda x: x[1], 
                          reverse=True)[:5]
for feature, importance in sorted_importance:
    print(f"{feature}: {importance:.2f}")</div>
                    
                    <div class="code-explanation">
                        <h4>What This Code Does:</h4>
                        <p>This implementation creates an XGBoost classifier with advanced regularization and optimization features. The model trains up to 100 boosting rounds but uses early stopping to halt training if validation performance stops improving for 10 consecutive rounds. This prevents overfitting while maximizing performance. The DMatrix data structure is XGBoost's optimized format for storing data, which enables faster training compared to standard numpy arrays.</p>
                    </div>
                    
                    <div class="code-explanation">
                        <h4>How It Works:</h4>
                        <p>XGBoost builds on Gradient Boosting by adding regularization terms to the objective function. The reg_alpha parameter applies L1 regularization (encouraging sparsity), while reg_lambda applies L2 regularization (encouraging small weights). These regularization terms penalize model complexity, helping prevent overfitting. The colsample_bytree parameter randomly samples 80% of features when building each tree, which introduces additional randomness and reduces overfitting. The min_child_weight parameter prevents the algorithm from creating leaves with too few examples, which would likely represent noise rather than true patterns. The early stopping mechanism monitors the evaluation metric (AUC in this case) on the test set and stops training when performance plateaus, automatically finding the optimal number of trees.</p>
                    </div>
                    
                    <div class="why-box">
                        <h4>Why We Code It This Way:</h4>
                        <p>The combination of subsample=0.8 and colsample_bytree=0.8 creates double randomness in the training process. Each tree sees only 80% of the training examples and only 80% of the features. This aggressive randomization prevents overfitting and often produces better generalization than models that use all data and features. The regularization parameters (reg_alpha=0.1 and reg_lambda=1.0) add explicit penalties for model complexity. These penalties discourage the algorithm from creating overly specific rules that fit training data perfectly but fail on new data.</p>
                        
                        <p>The early stopping mechanism is perhaps the most important parameter for practical applications. Rather than guessing how many trees you need, early stopping automatically determines this by monitoring validation performance. If the model stops improving for 10 consecutive rounds, training halts and the best iteration is returned. This approach saves computational resources while ensuring optimal performance. The eval_metric='auc' choice is deliberate for classification problems where you care about ranking predictions correctly (such as fraud detection or medical diagnosis), not just classification accuracy.</p>
                        
                        <p>Using the DMatrix format provides significant performance benefits. XGBoost's internal data structure is optimized for the operations required during tree building, including finding optimal split points and calculating gradients. This optimization, combined with parallel processing capabilities, allows XGBoost to train on datasets with millions of examples in reasonable time. The importance_type='gain' parameter tells XGBoost to measure feature importance by the average improvement in loss function when that feature is used for splitting. This provides more meaningful importance scores than simply counting how often a feature is used.</p>
                    </div>
                </div>
            </div>
            
            <!-- CASE STUDY -->
            <div class="section">
                <h2 class="section-title">Case Study: Predicting Customer Churn at TelecomCo</h2>
                
                <div class="real-world-example">
                    <h3>Business Context</h3>
                    <p>TelecomCo, a major telecommunications provider with 5 million customers, faces an annual customer churn rate of 27%. Each lost customer represents $1,800 in lifetime value, resulting in $2.4 billion in annual revenue loss. The company's retention team can only contact 50,000 customers per month with targeted retention offers. The challenge is identifying which customers are most likely to churn so the retention team can focus their efforts effectively.</p>
                    
                    <p>The previous approach used logistic regression, which achieved 72% accuracy but failed to identify many high-value customers who eventually churned. The cost of a false negative (missing a customer who will churn) is $1,800, while the cost of a false positive (offering retention incentives to a customer who would stay anyway) is $200. This asymmetry makes accuracy alone an insufficient metric. The business needs a model that prioritizes catching potential churners, even at the cost of some false positives.</p>
                </div>
                
                <h4>The Data</h4>
                
                <div class="concept-box">
                    <p>The dataset contains 100,000 customer records with 50 features including tenure, monthly charges, contract type, payment method, service usage patterns, customer service calls, and demographic information. The target variable indicates whether the customer churned within the next month. The dataset is imbalanced, with only 23% of customers having churned. This imbalance reflects the real-world challenge where churners are a minority but represent significant business impact.</p>
                </div>
                
                <h4>Solution Development</h4>
                
                <div class="code-block"># Import libraries and prepare data
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import precision_recall_curve, roc_auc_score, confusion_matrix
from sklearn.preprocessing import StandardScaler
import xgboost as xgb

# Load and prepare data
df = pd.read_csv('telecom_churn.csv')

# Separate features and target
X = df.drop(['customer_id', 'churn'], axis=1)
y = df['churn']

# Handle categorical variables
categorical_cols = X.select_dtypes(include=['object']).columns
X = pd.get_dummies(X, columns=categorical_cols, drop_first=True)

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Configure XGBoost for imbalanced data
scale_pos_weight = len(y_train[y_train==0]) / len(y_train[y_train==1])

params = {
    'objective': 'binary:logistic',
    'max_depth': 5,
    'learning_rate': 0.05,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'min_child_weight': 3,
    'gamma': 0.1,
    'reg_alpha': 0.1,
    'reg_lambda': 1.0,
    'scale_pos_weight': scale_pos_weight,
    'eval_metric': ['auc', 'logloss'],
    'seed': 42
}

# Prepare data in DMatrix format
dtrain = xgb.DMatrix(X_train_scaled, label=y_train)
dtest = xgb.DMatrix(X_test_scaled, label=y_test)

# Train with early stopping
evals = [(dtrain, 'train'), (dtest, 'eval')]
model = xgb.train(
    params,
    dtrain,
    num_boost_round=500,
    evals=evals,
    early_stopping_rounds=20,
    verbose_eval=50
)

# Make predictions
y_pred_proba = model.predict(dtest)

# Calculate business-aware threshold
# Cost of false negative: $1,800
# Cost of false positive: $200
cost_ratio = 1800 / 200
threshold = 1 / (1 + cost_ratio)

y_pred = (y_pred_proba >= threshold).astype(int)

# Evaluate results
cm = confusion_matrix(y_test, y_pred)
tn, fp, fn, tp = cm.ravel()

precision = tp / (tp + fp)
recall = tp / (tp + fn)
f1_score = 2 * (precision * recall) / (precision + recall)
auc_score = roc_auc_score(y_test, y_pred_proba)

# Calculate business metrics
saved_customers = tp
missed_churners = fn
unnecessary_offers = fp
cost_saved = saved_customers * 1800
wasted_incentives = unnecessary_offers * 200
net_benefit = cost_saved - wasted_incentives

print(f"Model Performance:")
print(f"AUC: {auc_score:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1_score:.4f}")
print(f"\nBusiness Impact:")
print(f"Customers Saved: {saved_customers:,}")
print(f"Revenue Retained: ${cost_saved:,}")
print(f"Incentive Cost: ${wasted_incentives:,}")
print(f"Net Benefit: ${net_benefit:,}")</div>
                
                <div class="code-explanation">
                    <h4>Key Implementation Decisions:</h4>
                    <p>The scale_pos_weight parameter addresses the class imbalance by giving higher importance to the minority class (churners). This parameter is calculated as the ratio of negative examples to positive examples, which tells XGBoost to treat each churner as equivalent to multiple non-churners during training. This approach ensures the model does not simply predict that everyone will stay, which would achieve high accuracy but miss most churners.</p>
                    
                    <p>The business-aware threshold calculation represents a critical insight. Rather than using the default 0.5 threshold, we calculate an optimal threshold based on business costs. When the cost of missing a churner ($1,800) far exceeds the cost of offering unnecessary retention incentives ($200), the optimal decision boundary shifts to favor predicting churn more aggressively. This threshold of approximately 0.1 means we classify a customer as likely to churn if the model assigns even a 10% probability to that outcome.</p>
                </div>
                
                <h4>Results and Business Impact</h4>
                
                <div class="real-world-example">
                    <p>The XGBoost model achieved an AUC of 0.89, significantly outperforming the previous logistic regression model (AUC 0.76). More importantly, the business-aware threshold optimization led to identifying 8,200 of the 4,600 actual churners in the test set (82% recall), while flagging 12,000 non-churners for retention offers (resulting in 41% precision). The net business impact was substantial. The model enabled the company to save $14.76 million in customer lifetime value by preventing 8,200 customers from churning, at a cost of only $2.4 million in retention incentives offered to customers who would have stayed anyway. This yielded a net benefit of $12.36 million annually.</p>
                    
                    <p>The feature importance analysis revealed that tenure, number of customer service calls in the last month, and monthly charges were the top three predictors of churn. Customers with less than 6 months tenure who made more than 3 customer service calls had a 67% churn probability. This insight allowed the customer success team to implement proactive outreach programs for new customers experiencing service issues, further reducing churn beyond the model's direct impact.</p>
                </div>
                
                <div class="key-insight">
                    <p><strong>Critical Lessons:</strong> This case study demonstrates several key principles of applying boosting algorithms to business problems. First, handling class imbalance through the scale_pos_weight parameter ensures the model pays adequate attention to the minority class. Second, optimizing the decision threshold based on business costs rather than using default values dramatically improves business outcomes. Third, feature importance analysis provides actionable insights beyond simple predictions. Fourth, the sequential nature of boosting allows the model to focus on hard-to-classify customers, which in churn prediction are often the most valuable customers to save. Finally, the regularization and early stopping mechanisms in XGBoost prevent overfitting, ensuring the model generalizes well to new customers rather than memorizing patterns specific to the training data.</p>
                </div>
            </div>
            
        </div>
    </div>
    
    <script>
        // Interactive visualization code
        let boostingChart = null;
        let comparisonChart = null;
        
        // Update slider values display
        document.getElementById('numEstimators').addEventListener('input', function() {
            document.getElementById('estimatorValue').textContent = this.value;
        });
        
        document.getElementById('learningRate').addEventListener('input', function() {
            document.getElementById('learningRateValue').textContent = parseFloat(this.value).toFixed(2);
        });
        
        document.getElementById('maxDepth').addEventListener('input', function() {
            document.getElementById('maxDepthValue').textContent = this.value;
        });
        
        // Generate synthetic data for visualization
        function generateData(numPoints = 200) {
            const data = [];
            for (let i = 0; i < numPoints; i++) {
                const x = Math.random() * 10 - 5;
                const y = Math.random() * 10 - 5;
                const label = (x * x + y * y < 16 && x + y > 0) ? 1 : 0;
                const noise = Math.random() * 0.3;
                data.push({ x: x + noise, y: y + noise, label });
            }
            return data;
        }
        
        // Train and visualize boosting
        function trainBoostingModel() {
            const numEstimators = parseInt(document.getElementById('numEstimators').value);
            const learningRate = parseFloat(document.getElementById('learningRate').value);
            const maxDepth = parseInt(document.getElementById('maxDepth').value);
            
            const data = generateData();
            
            // Simulate boosting training (simplified for visualization)
            const trainAccuracy = 0.85 + (numEstimators / 100) * 0.1 - (maxDepth / 20) * 0.02;
            const testAccuracy = 0.82 + (numEstimators / 150) * 0.08 - (maxDepth / 15) * 0.03;
            
            document.getElementById('trainAccuracy').textContent = (trainAccuracy * 100).toFixed(2) + '%';
            document.getElementById('testAccuracy').textContent = (testAccuracy * 100).toFixed(2) + '%';
            document.getElementById('iterations').textContent = numEstimators;
            
            // Visualize decision boundary
            const ctx = document.getElementById('boostingVisualization').getContext('2d');
            
            if (boostingChart) {
                boostingChart.destroy();
            }
            
            const class0 = data.filter(d => d.label === 0);
            const class1 = data.filter(d => d.label === 1);
            
            boostingChart = new Chart(ctx, {
                type: 'scatter',
                data: {
                    datasets: [
                        {
                            label: 'Class 0 (Non-Churn)',
                            data: class0.map(d => ({x: d.x, y: d.y})),
                            backgroundColor: 'rgba(102, 126, 234, 0.6)',
                            pointRadius: 5
                        },
                        {
                            label: 'Class 1 (Churn)',
                            data: class1.map(d => ({x: d.x, y: d.y})),
                            backgroundColor: 'rgba(255, 99, 132, 0.6)',
                            pointRadius: 5
                        }
                    ]
                },
                options: {
                    responsive: true,
                    maintainAspectRatio: false,
                    scales: {
                        x: {
                            type: 'linear',
                            position: 'bottom',
                            title: { display: true, text: 'Feature 1' }
                        },
                        y: {
                            title: { display: true, text: 'Feature 2' }
                        }
                    },
                    plugins: {
                        title: {
                            display: true,
                            text: `Boosting Classification (${numEstimators} estimators, lr=${learningRate}, depth=${maxDepth})`,
                            font: { size: 16 }
                        },
                        legend: {
                            display: true,
                            position: 'top'
                        }
                    }
                }
            });
        }
        
        // Show Random Forest vs Boosting comparison
        function showComparison() {
            const ctx = document.getElementById('comparisonChart').getContext('2d');
            
            if (comparisonChart) {
                comparisonChart.destroy();
            }
            
            // Simulated performance data across different dataset sizes
            const datasetSizes = [100, 500, 1000, 5000, 10000, 50000];
            const rfAccuracy = [0.82, 0.85, 0.87, 0.88, 0.89, 0.89];
            const boostingAccuracy = [0.78, 0.86, 0.89, 0.91, 0.93, 0.94];
            
            comparisonChart = new Chart(ctx, {
                type: 'line',
                data: {
                    labels: datasetSizes,
                    datasets: [
                        {
                            label: 'Random Forest',
                            data: rfAccuracy,
                            borderColor: 'rgba(75, 192, 192, 1)',
                            backgroundColor: 'rgba(75, 192, 192, 0.2)',
                            tension: 0.4
                        },
                        {
                            label: 'Gradient Boosting',
                            data: boostingAccuracy,
                            borderColor: 'rgba(153, 102, 255, 1)',
                            backgroundColor: 'rgba(153, 102, 255, 0.2)',
                            tension: 0.4
                        }
                    ]
                },
                options: {
                    responsive: true,
                    scales: {
                        x: {
                            title: { display: true, text: 'Dataset Size' }
                        },
                        y: {
                            title: { display: true, text: 'Accuracy' },
                            min: 0.7,
                            max: 1.0
                        }
                    },
                    plugins: {
                        title: {
                            display: true,
                            text: 'Random Forest vs Gradient Boosting Performance',
                            font: { size: 16 }
                        },
                        legend: {
                            display: true,
                            position: 'top'
                        }
                    }
                }
            });
        }
    </script>
</body>
</html>
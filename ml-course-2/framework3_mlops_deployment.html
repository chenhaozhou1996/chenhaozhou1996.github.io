<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Framework 3: MLOps & Model Deployment | Chenhao Zhou</title>
    <meta name="description" content="Production model deployment, containerization, serving patterns, monitoring, and MLOps best practices.">
    <style>
        :root {
            --primary: #10b981;
            --primary-dark: #059669;
            --primary-gradient: linear-gradient(135deg, #10b981 0%, #059669 100%);
            --text-primary: #1a1a1a;
            --text-secondary: #4a4a4a;
            --text-muted: #6b7280;
            --background: #ffffff;
            --surface: #f8fafc;
            --border: #e5e7eb;
            --code-bg: #1e293b;
        }

        * { margin: 0; padding: 0; box-sizing: border-box; }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', sans-serif;
            background: var(--background);
            color: var(--text-primary);
            line-height: 1.7;
        }

        .hero {
            background: var(--primary-gradient);
            padding: 60px 20px 80px;
            color: white;
        }

        .container { max-width: 1000px; margin: 0 auto; padding: 0 20px; }

        .breadcrumb { margin-bottom: 20px; opacity: 0.9; }
        .breadcrumb a { color: white; text-decoration: none; }
        .breadcrumb a:hover { text-decoration: underline; }

        .hero h1 { font-size: clamp(2rem, 4vw, 3rem); margin-bottom: 16px; }
        .hero-meta { display: flex; gap: 24px; flex-wrap: wrap; opacity: 0.9; }

        .nav-wrapper {
            position: sticky; top: 0; z-index: 100;
            background: rgba(255,255,255,0.95);
            backdrop-filter: blur(10px);
            border-bottom: 1px solid var(--border);
        }

        .nav-bar {
            max-width: 1000px; margin: 0 auto; padding: 12px 20px;
            display: flex; gap: 8px; flex-wrap: wrap;
        }

        .nav-link {
            color: var(--text-secondary); text-decoration: none;
            padding: 8px 16px; border-radius: 6px; font-size: 0.9rem; transition: all 0.2s;
        }
        .nav-link:hover { background: var(--primary); color: white; }

        .content { padding: 48px 20px; }
        .section { margin-bottom: 64px; }

        .section-number {
            display: inline-block; background: var(--primary-gradient);
            color: white; width: 36px; height: 36px; border-radius: 50%;
            text-align: center; line-height: 36px; font-weight: 700; margin-right: 12px;
        }

        h2 { font-size: 1.75rem; margin-bottom: 24px; display: flex; align-items: center; }
        h3 { font-size: 1.25rem; margin: 32px 0 16px; color: var(--primary-dark); }
        h4 { font-size: 1.1rem; margin: 24px 0 12px; }
        p { margin-bottom: 16px; color: var(--text-secondary); }

        .concept-box {
            background: linear-gradient(135deg, #ecfdf5 0%, #d1fae5 100%);
            border-left: 4px solid var(--primary);
            padding: 24px; border-radius: 0 12px 12px 0; margin: 24px 0;
        }
        .concept-box h4 { color: var(--primary-dark); margin-top: 0; }

        .code-block {
            background: var(--code-bg); border-radius: 12px;
            padding: 24px; margin: 24px 0; overflow-x: auto;
        }
        .code-block pre {
            color: #e2e8f0; font-family: 'JetBrains Mono', 'Fira Code', monospace;
            font-size: 0.9rem; line-height: 1.6; margin: 0;
        }
        .code-label {
            color: #94a3b8; font-size: 0.8rem; margin-bottom: 12px;
            text-transform: uppercase; letter-spacing: 0.5px;
        }

        .diagram {
            background: var(--surface); border: 1px solid var(--border);
            border-radius: 12px; padding: 32px; margin: 24px 0; text-align: center;
        }
        .diagram-title { font-weight: 600; margin-bottom: 20px; }

        .comparison-table {
            width: 100%; border-collapse: collapse; margin: 24px 0; font-size: 0.95rem;
        }
        .comparison-table th, .comparison-table td {
            padding: 16px; text-align: left; border-bottom: 1px solid var(--border);
        }
        .comparison-table th { background: var(--surface); font-weight: 600; }
        .comparison-table tr:hover { background: #f8fafc; }

        .feature-grid {
            display: grid; grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
            gap: 20px; margin: 24px 0;
        }
        .feature-card {
            background: white; border: 1px solid var(--border);
            border-radius: 12px; padding: 24px; transition: all 0.2s;
        }
        .feature-card:hover { box-shadow: 0 4px 12px rgba(0,0,0,0.08); transform: translateY(-2px); }
        .feature-title { font-weight: 600; margin-bottom: 8px; }
        .feature-desc { font-size: 0.9rem; color: var(--text-secondary); }

        .key-points {
            background: #fef3c7; border-radius: 12px; padding: 24px; margin: 24px 0;
        }
        .key-points h4 { color: #92400e; margin-top: 0; }
        .key-points ul { margin: 0; padding-left: 20px; }
        .key-points li { margin-bottom: 8px; color: #78350f; }

        .exercise {
            background: linear-gradient(135deg, #dbeafe 0%, #bfdbfe 100%);
            border-radius: 12px; padding: 24px; margin: 32px 0;
        }
        .exercise h4 { color: #1e40af; margin-top: 0; }
        .exercise p, .exercise li { color: #1e3a8a; }

        .footer {
            background: var(--text-primary); color: white;
            padding: 32px 20px; text-align: center;
        }
        .footer a { color: rgba(255,255,255,0.8); text-decoration: none; margin: 0 16px; }
        .footer a:hover { color: white; }
    </style>
</head>
<body>
    <section class="hero">
        <div class="container">
            <div class="breadcrumb">
                <a href="index.html">ML Frameworks Course</a> / Framework 3
            </div>
            <h1>MLOps & Model Deployment</h1>
            <div class="hero-meta">
                <span>Framework 3 of 4</span>
                <span>|</span>
                <span>Estimated: 6-8 hours</span>
                <span>|</span>
                <span>Level: Advanced</span>
            </div>
        </div>
    </section>

    <div class="nav-wrapper">
        <nav class="nav-bar">
            <a href="#containerization" class="nav-link">Containerization</a>
            <a href="#serving" class="nav-link">Serving Patterns</a>
            <a href="#deployment" class="nav-link">Deployment Strategies</a>
            <a href="#monitoring" class="nav-link">Monitoring</a>
            <a href="#observability" class="nav-link">Observability</a>
            <a href="index.html" class="nav-link">Back to Course</a>
        </nav>
    </div>

    <main class="content">
        <div class="container">

            <!-- Section 1: Containerization -->
            <section id="containerization" class="section">
                <h2><span class="section-number">1</span>Containerization for ML</h2>

                <p>Containerization packages ML models with their dependencies into portable, reproducible units. Docker containers ensure consistent behavior across development, testing, and production environments.</p>

                <h3>1.1 Dockerfile for ML Model</h3>

                <div class="code-block">
                    <div class="code-label">Dockerfile - ML Prediction Service</div>
                    <pre>
# Multi-stage build for smaller image
FROM python:3.10-slim as builder

WORKDIR /app

# Install build dependencies
RUN apt-get update && apt-get install -y \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
COPY requirements.txt .
RUN pip wheel --no-cache-dir --wheel-dir /wheels -r requirements.txt

# Production stage
FROM python:3.10-slim

WORKDIR /app

# Install runtime dependencies only
RUN apt-get update && apt-get install -y \
    libgomp1 \
    && rm -rf /var/lib/apt/lists/*

# Copy wheels from builder
COPY --from=builder /wheels /wheels
RUN pip install --no-cache-dir /wheels/* && rm -rf /wheels

# Copy application code
COPY src/ ./src/
COPY models/ ./models/
COPY config/ ./config/

# Set environment variables
ENV PYTHONUNBUFFERED=1
ENV MODEL_PATH=/app/models/model.joblib
ENV PORT=8080

# Create non-root user
RUN useradd -m appuser && chown -R appuser:appuser /app
USER appuser

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:${PORT}/health || exit 1

# Run the application
EXPOSE 8080
CMD ["uvicorn", "src.api:app", "--host", "0.0.0.0", "--port", "8080"]</pre>
                </div>

                <h3>1.2 Docker Compose for Local Development</h3>

                <div class="code-block">
                    <div class="code-label">docker-compose.yml</div>
                    <pre>
version: '3.8'

services:
  prediction-service:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "8080:8080"
    environment:
      - MODEL_PATH=/app/models/model.joblib
      - LOG_LEVEL=INFO
      - REDIS_HOST=redis
    volumes:
      - ./models:/app/models:ro
    depends_on:
      - redis
      - prometheus
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data

  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana-data:/var/lib/grafana

volumes:
  redis-data:
  grafana-data:</pre>
                </div>

                <h3>1.3 Kubernetes Deployment</h3>

                <div class="code-block">
                    <div class="code-label">kubernetes/deployment.yaml</div>
                    <pre>
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ml-prediction-service
  labels:
    app: ml-prediction
spec:
  replicas: 3
  selector:
    matchLabels:
      app: ml-prediction
  template:
    metadata:
      labels:
        app: ml-prediction
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8080"
        prometheus.io/path: "/metrics"
    spec:
      containers:
      - name: prediction-service
        image: gcr.io/my-project/ml-service:v1.2.0
        ports:
        - containerPort: 8080
        env:
        - name: MODEL_VERSION
          value: "v1.2.0"
        - name: LOG_LEVEL
          value: "INFO"
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
        volumeMounts:
        - name: model-cache
          mountPath: /app/cache
      volumes:
      - name: model-cache
        emptyDir:
          sizeLimit: 1Gi
---
apiVersion: v1
kind: Service
metadata:
  name: ml-prediction-service
spec:
  selector:
    app: ml-prediction
  ports:
  - port: 80
    targetPort: 8080
  type: ClusterIP
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: ml-prediction-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: ml-prediction-service
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Pods
    pods:
      metric:
        name: prediction_latency_p99
      target:
        type: AverageValue
        averageValue: 100m</pre>
                </div>
            </section>

            <!-- Section 2: Serving Patterns -->
            <section id="serving" class="section">
                <h2><span class="section-number">2</span>Model Serving Patterns</h2>

                <p>Different use cases require different serving patterns. Understanding the trade-offs helps you choose the right approach for your requirements.</p>

                <h3>2.1 Serving Pattern Comparison</h3>

                <table class="comparison-table">
                    <thead>
                        <tr>
                            <th>Pattern</th>
                            <th>Latency</th>
                            <th>Throughput</th>
                            <th>Use Case</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>Real-time</strong></td>
                            <td>&lt; 100ms</td>
                            <td>Variable</td>
                            <td>User-facing features, fraud detection</td>
                        </tr>
                        <tr>
                            <td><strong>Batch</strong></td>
                            <td>Minutes-hours</td>
                            <td>Very high</td>
                            <td>Recommendations, scoring pipelines</td>
                        </tr>
                        <tr>
                            <td><strong>Streaming</strong></td>
                            <td>Seconds</td>
                            <td>High</td>
                            <td>Real-time analytics, anomaly detection</td>
                        </tr>
                        <tr>
                            <td><strong>Embedded</strong></td>
                            <td>&lt; 10ms</td>
                            <td>N/A</td>
                            <td>Mobile apps, edge devices</td>
                        </tr>
                    </tbody>
                </table>

                <h3>2.2 Real-time Serving Implementation</h3>

                <div class="code-block">
                    <div class="code-label">Python - FastAPI with Caching</div>
                    <pre>
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import redis
import hashlib
import json
import mlflow.pyfunc
from typing import List
import asyncio

app = FastAPI()

# Initialize Redis for caching
redis_client = redis.Redis(host='redis', port=6379, decode_responses=True)
CACHE_TTL = 3600  # 1 hour

# Load model at startup
model = None

@app.on_event("startup")
async def load_model():
    global model
    model = mlflow.pyfunc.load_model("models:/churn_model/Production")

class PredictionRequest(BaseModel):
    customer_id: str
    features: dict

class PredictionResponse(BaseModel):
    customer_id: str
    prediction: float
    cached: bool

def get_cache_key(features: dict) -> str:
    """Generate cache key from features."""
    feature_str = json.dumps(features, sort_keys=True)
    return f"pred:{hashlib.md5(feature_str.encode()).hexdigest()}"

@app.post("/predict", response_model=PredictionResponse)
async def predict(request: PredictionRequest):
    # Check cache first
    cache_key = get_cache_key(request.features)
    cached_result = redis_client.get(cache_key)

    if cached_result:
        return PredictionResponse(
            customer_id=request.customer_id,
            prediction=float(cached_result),
            cached=True
        )

    # Run prediction
    import pandas as pd
    features_df = pd.DataFrame([request.features])
    prediction = model.predict(features_df)[0]

    # Cache result
    redis_client.setex(cache_key, CACHE_TTL, str(prediction))

    return PredictionResponse(
        customer_id=request.customer_id,
        prediction=float(prediction),
        cached=False
    )

@app.post("/predict/batch", response_model=List[PredictionResponse])
async def predict_batch(requests: List[PredictionRequest]):
    """Batch prediction with parallel processing."""
    import pandas as pd

    # Separate cached and uncached
    results = {}
    uncached = []

    for req in requests:
        cache_key = get_cache_key(req.features)
        cached = redis_client.get(cache_key)
        if cached:
            results[req.customer_id] = (float(cached), True)
        else:
            uncached.append(req)

    # Batch predict uncached
    if uncached:
        features_df = pd.DataFrame([r.features for r in uncached])
        predictions = model.predict(features_df)

        for req, pred in zip(uncached, predictions):
            cache_key = get_cache_key(req.features)
            redis_client.setex(cache_key, CACHE_TTL, str(pred))
            results[req.customer_id] = (float(pred), False)

    return [
        PredictionResponse(customer_id=cid, prediction=pred, cached=cached)
        for cid, (pred, cached) in results.items()
    ]</pre>
                </div>

                <h3>2.3 Batch Serving Implementation</h3>

                <div class="code-block">
                    <div class="code-label">Python - Batch Prediction Pipeline</div>
                    <pre>
from pyspark.sql import SparkSession
from pyspark.sql.functions import pandas_udf, col
import pandas as pd
import mlflow.pyfunc

spark = SparkSession.builder \
    .appName("BatchPrediction") \
    .config("spark.sql.execution.arrow.pyspark.enabled", "true") \
    .getOrCreate()

# Load model
model = mlflow.pyfunc.load_model("models:/churn_model/Production")

# Broadcast model to workers
model_broadcast = spark.sparkContext.broadcast(model)

@pandas_udf("double")
def predict_udf(
    feature_a: pd.Series,
    feature_b: pd.Series,
    feature_c: pd.Series
) -> pd.Series:
    """UDF for distributed prediction."""
    model = model_broadcast.value
    features = pd.DataFrame({
        'feature_a': feature_a,
        'feature_b': feature_b,
        'feature_c': feature_c
    })
    return pd.Series(model.predict(features))

def run_batch_prediction(input_path: str, output_path: str):
    """Run batch predictions on large dataset."""
    # Read input data
    df = spark.read.parquet(input_path)

    # Add predictions
    df_with_predictions = df.withColumn(
        "churn_probability",
        predict_udf(col("feature_a"), col("feature_b"), col("feature_c"))
    )

    # Write results
    df_with_predictions.write \
        .mode("overwrite") \
        .partitionBy("date") \
        .parquet(output_path)

    return df_with_predictions.count()

# Run
num_predictions = run_batch_prediction(
    "s3://data/customers/",
    "s3://predictions/churn/"
)</pre>
                </div>
            </section>

            <!-- Section 3: Deployment Strategies -->
            <section id="deployment" class="section">
                <h2><span class="section-number">3</span>Deployment Strategies</h2>

                <p>Deploying ML models safely requires strategies that minimize risk and enable quick rollback. These patterns help manage model releases in production.</p>

                <div class="feature-grid">
                    <div class="feature-card">
                        <div class="feature-title">Blue-Green Deployment</div>
                        <div class="feature-desc">Maintain two identical environments. Switch traffic instantly between them for zero-downtime deployments.</div>
                    </div>
                    <div class="feature-card">
                        <div class="feature-title">Canary Release</div>
                        <div class="feature-desc">Gradually route traffic to new model version. Monitor for issues before full rollout.</div>
                    </div>
                    <div class="feature-card">
                        <div class="feature-title">Shadow Mode</div>
                        <div class="feature-desc">Run new model in parallel without serving results. Compare outputs to production model.</div>
                    </div>
                    <div class="feature-card">
                        <div class="feature-title">A/B Testing</div>
                        <div class="feature-desc">Split traffic between model versions. Measure business metrics to determine winner.</div>
                    </div>
                </div>

                <h3>3.1 Canary Deployment with Kubernetes</h3>

                <div class="code-block">
                    <div class="code-label">kubernetes/canary-deployment.yaml</div>
                    <pre>
# Stable deployment (90% traffic)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ml-service-stable
  labels:
    app: ml-service
    version: stable
spec:
  replicas: 9
  selector:
    matchLabels:
      app: ml-service
      version: stable
  template:
    metadata:
      labels:
        app: ml-service
        version: stable
    spec:
      containers:
      - name: ml-service
        image: gcr.io/project/ml-service:v1.0.0
        env:
        - name: MODEL_VERSION
          value: "v1.0.0"
---
# Canary deployment (10% traffic)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ml-service-canary
  labels:
    app: ml-service
    version: canary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ml-service
      version: canary
  template:
    metadata:
      labels:
        app: ml-service
        version: canary
    spec:
      containers:
      - name: ml-service
        image: gcr.io/project/ml-service:v1.1.0
        env:
        - name: MODEL_VERSION
          value: "v1.1.0"
---
# Service routes to both
apiVersion: v1
kind: Service
metadata:
  name: ml-service
spec:
  selector:
    app: ml-service  # Selects both stable and canary
  ports:
  - port: 80
    targetPort: 8080</pre>
                </div>

                <h3>3.2 A/B Testing Framework</h3>

                <div class="code-block">
                    <div class="code-label">Python - A/B Test Router</div>
                    <pre>
from fastapi import FastAPI, Request
from dataclasses import dataclass
from typing import Dict
import hashlib
import mlflow.pyfunc

app = FastAPI()

@dataclass
class ModelVariant:
    name: str
    model: any
    traffic_percent: float

class ABTestRouter:
    def __init__(self):
        self.variants: Dict[str, ModelVariant] = {}
        self.total_traffic = 0.0

    def register_variant(self, name: str, model_uri: str, traffic_percent: float):
        model = mlflow.pyfunc.load_model(model_uri)
        self.variants[name] = ModelVariant(name, model, traffic_percent)
        self.total_traffic = sum(v.traffic_percent for v in self.variants.values())

    def get_variant(self, user_id: str) -> ModelVariant:
        """Deterministic variant assignment based on user_id."""
        hash_value = int(hashlib.md5(user_id.encode()).hexdigest(), 16)
        bucket = (hash_value % 100) / 100.0

        cumulative = 0.0
        for variant in self.variants.values():
            cumulative += variant.traffic_percent / self.total_traffic
            if bucket < cumulative:
                return variant

        return list(self.variants.values())[-1]

# Initialize router
router = ABTestRouter()
router.register_variant("control", "models:/churn_model/v1", 0.5)
router.register_variant("treatment", "models:/churn_model/v2", 0.5)

@app.post("/predict")
async def predict(request: Request, user_id: str, features: dict):
    # Get assigned variant
    variant = router.get_variant(user_id)

    # Make prediction
    import pandas as pd
    prediction = variant.model.predict(pd.DataFrame([features]))[0]

    # Log for analysis
    log_experiment_event(
        user_id=user_id,
        variant=variant.name,
        prediction=prediction
    )

    return {
        "prediction": prediction,
        "variant": variant.name
    }

def log_experiment_event(user_id: str, variant: str, prediction: float):
    """Log event for A/B test analysis."""
    # Send to analytics system (Amplitude, Mixpanel, etc.)
    pass</pre>
                </div>
            </section>

            <!-- Section 4: Monitoring -->
            <section id="monitoring" class="section">
                <h2><span class="section-number">4</span>Model Monitoring</h2>

                <p>Production models require continuous monitoring to detect performance degradation, data drift, and system issues before they impact business outcomes.</p>

                <div class="concept-box">
                    <h4>What to Monitor</h4>
                    <ul>
                        <li><strong>Model metrics:</strong> Prediction accuracy, precision, recall (when labels available)</li>
                        <li><strong>Data drift:</strong> Changes in input feature distributions</li>
                        <li><strong>Concept drift:</strong> Changes in relationship between features and target</li>
                        <li><strong>System metrics:</strong> Latency, throughput, error rates</li>
                        <li><strong>Business metrics:</strong> Conversion rate, revenue impact</li>
                    </ul>
                </div>

                <h3>4.1 Prometheus Metrics Integration</h3>

                <div class="code-block">
                    <div class="code-label">Python - Model Monitoring Metrics</div>
                    <pre>
from prometheus_client import Counter, Histogram, Gauge, generate_latest
from fastapi import FastAPI, Response
import time

app = FastAPI()

# Define metrics
PREDICTION_COUNTER = Counter(
    'model_predictions_total',
    'Total number of predictions',
    ['model_version', 'prediction_class']
)

PREDICTION_LATENCY = Histogram(
    'model_prediction_latency_seconds',
    'Time spent processing prediction',
    ['model_version'],
    buckets=[0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0]
)

PREDICTION_CONFIDENCE = Histogram(
    'model_prediction_confidence',
    'Distribution of prediction confidence scores',
    ['model_version'],
    buckets=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
)

FEATURE_VALUE = Histogram(
    'model_feature_values',
    'Distribution of input feature values',
    ['feature_name'],
    buckets=[-3, -2, -1, 0, 1, 2, 3]  # For standardized features
)

MODEL_LOADED = Gauge(
    'model_loaded_timestamp',
    'Timestamp when model was loaded',
    ['model_version']
)

class MonitoredPredictor:
    def __init__(self, model, model_version: str):
        self.model = model
        self.model_version = model_version
        MODEL_LOADED.labels(model_version=model_version).set(time.time())

    def predict(self, features: dict) -> dict:
        start_time = time.time()

        # Record feature distributions
        for name, value in features.items():
            if isinstance(value, (int, float)):
                FEATURE_VALUE.labels(feature_name=name).observe(value)

        # Make prediction
        import pandas as pd
        prediction = self.model.predict(pd.DataFrame([features]))[0]
        confidence = self.model.predict_proba(pd.DataFrame([features]))[0].max()

        # Record metrics
        latency = time.time() - start_time
        PREDICTION_LATENCY.labels(model_version=self.model_version).observe(latency)
        PREDICTION_CONFIDENCE.labels(model_version=self.model_version).observe(confidence)

        pred_class = "positive" if prediction > 0.5 else "negative"
        PREDICTION_COUNTER.labels(
            model_version=self.model_version,
            prediction_class=pred_class
        ).inc()

        return {
            "prediction": float(prediction),
            "confidence": float(confidence),
            "latency_ms": latency * 1000
        }

@app.get("/metrics")
async def metrics():
    return Response(generate_latest(), media_type="text/plain")</pre>
                </div>

                <h3>4.2 Data Drift Monitoring</h3>

                <div class="code-block">
                    <div class="code-label">Python - Drift Detection Service</div>
                    <pre>
from scipy import stats
import numpy as np
from typing import Dict, List
from dataclasses import dataclass
from prometheus_client import Gauge

# Prometheus metrics for drift
DRIFT_SCORE = Gauge(
    'model_feature_drift_score',
    'KS statistic for feature drift',
    ['feature_name']
)

DRIFT_DETECTED = Gauge(
    'model_drift_detected',
    'Whether drift was detected (1) or not (0)',
    ['feature_name']
)

@dataclass
class FeatureStats:
    mean: float
    std: float
    percentiles: List[float]
    sample_values: np.ndarray

class DriftMonitor:
    def __init__(self, reference_stats: Dict[str, FeatureStats], threshold: float = 0.1):
        self.reference_stats = reference_stats
        self.threshold = threshold
        self.current_window: Dict[str, List[float]] = {k: [] for k in reference_stats}
        self.window_size = 1000

    def record_features(self, features: Dict[str, float]):
        """Record features for drift detection."""
        for name, value in features.items():
            if name in self.current_window:
                self.current_window[name].append(value)

                # Check drift when window is full
                if len(self.current_window[name]) >= self.window_size:
                    self._check_drift(name)
                    self.current_window[name] = []

    def _check_drift(self, feature_name: str):
        """Check for drift in a feature."""
        reference = self.reference_stats[feature_name].sample_values
        current = np.array(self.current_window[feature_name])

        # Kolmogorov-Smirnov test
        ks_stat, p_value = stats.ks_2samp(reference, current)

        # Update metrics
        DRIFT_SCORE.labels(feature_name=feature_name).set(ks_stat)
        drift_detected = ks_stat > self.threshold
        DRIFT_DETECTED.labels(feature_name=feature_name).set(1 if drift_detected else 0)

        if drift_detected:
            self._alert_drift(feature_name, ks_stat, p_value)

    def _alert_drift(self, feature_name: str, ks_stat: float, p_value: float):
        """Send drift alert."""
        # Implementation: Send to Slack, PagerDuty, etc.
        print(f"DRIFT ALERT: {feature_name} KS={ks_stat:.4f} p={p_value:.4f}")</pre>
                </div>
            </section>

            <!-- Section 5: Observability -->
            <section id="observability" class="section">
                <h2><span class="section-number">5</span>ML System Observability</h2>

                <p>Observability goes beyond monitoring to provide deep insight into system behavior. It combines logging, metrics, and tracing for comprehensive visibility.</p>

                <h3>5.1 Structured Logging</h3>

                <div class="code-block">
                    <div class="code-label">Python - Structured ML Logging</div>
                    <pre>
import structlog
import uuid
from datetime import datetime

# Configure structured logging
structlog.configure(
    processors=[
        structlog.stdlib.filter_by_level,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.JSONRenderer()
    ],
    context_class=dict,
    logger_factory=structlog.stdlib.LoggerFactory(),
)

logger = structlog.get_logger()

class PredictionLogger:
    def __init__(self, model_version: str):
        self.model_version = model_version

    def log_prediction(
        self,
        request_id: str,
        user_id: str,
        features: dict,
        prediction: float,
        latency_ms: float
    ):
        logger.info(
            "prediction_made",
            request_id=request_id,
            user_id=user_id,
            model_version=self.model_version,
            prediction=prediction,
            latency_ms=latency_ms,
            feature_count=len(features),
            timestamp=datetime.utcnow().isoformat()
        )

    def log_error(self, request_id: str, error: Exception):
        logger.error(
            "prediction_failed",
            request_id=request_id,
            model_version=self.model_version,
            error_type=type(error).__name__,
            error_message=str(error)
        )</pre>
                </div>

                <h3>5.2 Distributed Tracing</h3>

                <div class="code-block">
                    <div class="code-label">Python - OpenTelemetry Integration</div>
                    <pre>
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor

# Configure tracing
trace.set_tracer_provider(TracerProvider())
tracer = trace.get_tracer(__name__)

# Export to collector
otlp_exporter = OTLPSpanExporter(endpoint="http://otel-collector:4317")
trace.get_tracer_provider().add_span_processor(
    BatchSpanProcessor(otlp_exporter)
)

# Instrument FastAPI
from fastapi import FastAPI
app = FastAPI()
FastAPIInstrumentor.instrument_app(app)

@app.post("/predict")
async def predict(request: PredictionRequest):
    with tracer.start_as_current_span("predict") as span:
        span.set_attribute("user_id", request.user_id)
        span.set_attribute("model_version", MODEL_VERSION)

        # Feature retrieval
        with tracer.start_as_current_span("fetch_features"):
            features = await feature_store.get_features(request.user_id)
            span.set_attribute("feature_count", len(features))

        # Model inference
        with tracer.start_as_current_span("model_inference"):
            prediction = model.predict(features)
            span.set_attribute("prediction", prediction)

        return {"prediction": prediction}</pre>
                </div>

                <div class="exercise">
                    <h4>MLOps Deployment Checklist</h4>
                    <ul>
                        <li>Container image built with multi-stage Dockerfile</li>
                        <li>Kubernetes manifests with resource limits and health checks</li>
                        <li>Horizontal Pod Autoscaler configured</li>
                        <li>Prometheus metrics exposed for monitoring</li>
                        <li>Structured logging with request tracing</li>
                        <li>Drift detection pipeline running</li>
                        <li>Canary deployment strategy implemented</li>
                        <li>Rollback procedure documented and tested</li>
                    </ul>
                </div>
            </section>

        </div>
    </main>

    <footer class="footer">
        <a href="framework2_pipeline_engineering.html">Previous: Pipeline Engineering</a>
        <a href="index.html">Course Home</a>
        <a href="framework4_enterprise_integration.html">Next: Enterprise Integration</a>
        <p style="margin-top: 24px; opacity: 0.7;">Â© 2025 Chenhao Zhou</p>
    </footer>
</body>
</html>
